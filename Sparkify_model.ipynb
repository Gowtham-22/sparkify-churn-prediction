{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, OneHotEncoderEstimator, StringIndexer, VectorAssembler # PCA, IDF, \n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import IntegerType, StringType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = spark.read.json(\"data/mini_sparkify_event_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+---------------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|              artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|           page| registration|sessionId|                song|status|           ts|           userAgent|userId|\n",
      "+--------------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+---------------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|      Martha Tilston|Logged In|    Colin|     M|           50| Freeman|277.89016| paid|     Bakersfield, CA|   PUT|       NextSong|1538173362000|       29|           Rockpools|   200|1538352117000|Mozilla/5.0 (Wind...|    30|\n",
      "|    Five Iron Frenzy|Logged In|    Micah|     M|           79|    Long|236.09424| free|Boston-Cambridge-...|   PUT|       NextSong|1538331630000|        8|              Canada|   200|1538352180000|\"Mozilla/5.0 (Win...|     9|\n",
      "|        Adam Lambert|Logged In|    Colin|     M|           51| Freeman| 282.8273| paid|     Bakersfield, CA|   PUT|       NextSong|1538173362000|       29|   Time For Miracles|   200|1538352394000|Mozilla/5.0 (Wind...|    30|\n",
      "|              Enigma|Logged In|    Micah|     M|           80|    Long|262.71302| free|Boston-Cambridge-...|   PUT|       NextSong|1538331630000|        8|Knocking On Forbi...|   200|1538352416000|\"Mozilla/5.0 (Win...|     9|\n",
      "|           Daft Punk|Logged In|    Colin|     M|           52| Freeman|223.60771| paid|     Bakersfield, CA|   PUT|       NextSong|1538173362000|       29|Harder Better Fas...|   200|1538352676000|Mozilla/5.0 (Wind...|    30|\n",
      "|The All-American ...|Logged In|    Micah|     M|           81|    Long|208.29995| free|Boston-Cambridge-...|   PUT|       NextSong|1538331630000|        8|      Don't Leave Me|   200|1538352678000|\"Mozilla/5.0 (Win...|     9|\n",
      "|The Velvet Underg...|Logged In|    Micah|     M|           82|    Long|260.46649| free|Boston-Cambridge-...|   PUT|       NextSong|1538331630000|        8|         Run Run Run|   200|1538352886000|\"Mozilla/5.0 (Win...|     9|\n",
      "|        Starflyer 59|Logged In|    Colin|     M|           53| Freeman|185.44281| paid|     Bakersfield, CA|   PUT|       NextSong|1538173362000|       29|Passengers (Old A...|   200|1538352899000|Mozilla/5.0 (Wind...|    30|\n",
      "|                null|Logged In|    Colin|     M|           54| Freeman|     null| paid|     Bakersfield, CA|   PUT|Add to Playlist|1538173362000|       29|                null|   200|1538352905000|Mozilla/5.0 (Wind...|    30|\n",
      "|            Frumpies|Logged In|    Colin|     M|           55| Freeman|134.47791| paid|     Bakersfield, CA|   PUT|       NextSong|1538173362000|       29|          Fuck Kitty|   200|1538353084000|Mozilla/5.0 (Wind...|    30|\n",
      "|        Britt Nicole|Logged In|    Micah|     M|           83|    Long| 229.8771| free|Boston-Cambridge-...|   PUT|       NextSong|1538331630000|        8|   Walk On The Water|   200|1538353146000|\"Mozilla/5.0 (Win...|     9|\n",
      "|                null|Logged In|    Micah|     M|           84|    Long|     null| free|Boston-Cambridge-...|   GET|    Roll Advert|1538331630000|        8|                null|   200|1538353150000|\"Mozilla/5.0 (Win...|     9|\n",
      "|Edward Sharpe & T...|Logged In|    Colin|     M|           56| Freeman|223.58159| paid|     Bakersfield, CA|   PUT|       NextSong|1538173362000|       29|                Jade|   200|1538353218000|Mozilla/5.0 (Wind...|    30|\n",
      "|               Tesla|Logged In|    Micah|     M|           85|    Long|201.06404| free|Boston-Cambridge-...|   PUT|       NextSong|1538331630000|        8|      Gettin' Better|   200|1538353375000|\"Mozilla/5.0 (Win...|     9|\n",
      "|                null|Logged In|    Micah|     M|           86|    Long|     null| free|Boston-Cambridge-...|   PUT|      Thumbs Up|1538331630000|        8|                null|   307|1538353376000|\"Mozilla/5.0 (Win...|     9|\n",
      "|         Stan Mosley|Logged In|    Colin|     M|           57| Freeman|246.69995| paid|     Bakersfield, CA|   PUT|       NextSong|1538173362000|       29|   So-Called Friends|   200|1538353441000|Mozilla/5.0 (Wind...|    30|\n",
      "|Florence + The Ma...|Logged In|    Micah|     M|           87|    Long|168.64608| free|Boston-Cambridge-...|   PUT|       NextSong|1538331630000|        8| You've Got The Love|   200|1538353576000|\"Mozilla/5.0 (Win...|     9|\n",
      "|   Tokyo Police Club|Logged In|  Ashlynn|     F|            0|Williams| 166.1122| free|     Tallahassee, FL|   PUT|       NextSong|1537365219000|      217|Citizens Of Tomorrow|   200|1538353668000|\"Mozilla/5.0 (Mac...|    74|\n",
      "|             Orishas|Logged In|    Colin|     M|           58| Freeman|222.22322| paid|     Bakersfield, CA|   PUT|       NextSong|1538173362000|       29|           Represent|   200|1538353687000|Mozilla/5.0 (Wind...|    30|\n",
      "|             Ratatat|Logged In|    Micah|     M|           88|    Long|229.77261| free|Boston-Cambridge-...|   PUT|       NextSong|1538331630000|        8|              Swisha|   200|1538353744000|\"Mozilla/5.0 (Win...|     9|\n",
      "+--------------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+---------------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The userId contains values with an empty string. These entries need to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total events: 286500; number of valid events 278154\n",
      "Number of users: 225\n"
     ]
    }
   ],
   "source": [
    "# events where a userId is an empty string are not valid, remove these\n",
    "valid_events = events.where(col(\"userId\") != \"\")\n",
    "print(\"Number of total events: {}; number of valid events {}\".format(events.count(), valid_events.count()))\n",
    "print(\"Number of users: {}\".format(valid_events.select(\"userId\").distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# registration and ts can both be converted to timestamps\n",
    "get_date = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0), TimestampType()) # udf to convert to timestamp/date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_events = valid_events.withColumn(\"log_date\", get_date(col(\"ts\"))) # date when the log entry was done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column \"churn\" to the dataframe indicating that a cancellation was confirmed\n",
    "find_churn = udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_events = valid_events.withColumn(\"churn\", find_churn(col(\"page\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_length = valid_events.groupBy(\"userId\").agg({\"length\": \"mean\"}) \\\n",
    "    .withColumnRenamed(\"avg(length)\", \"avg_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some values can be set into relation to a certain time period, e.g. the duration the user is active\n",
    "# hence we need to create a df with all users and their active time period\n",
    "# first find the first and last log entry for each user and how much log entries exist per user (all actions)\n",
    "time_df = valid_events.groupBy([\"userId\"]) \\\n",
    "    .agg(F.sum(\"churn\").alias(\"churned\"), F.min(\"log_date\").alias(\"first_log\"),\n",
    "         F.max(\"log_date\").alias(\"last_log\"), F.count(\"page\").alias(\"log_counts\"), F.max(\"ts\").alias(\"last_ts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_difference(date_1, date_2):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    # difference between the dates\n",
    "    delta = date_2 - date_1\n",
    "    \n",
    "    # minimum difference is one 1\n",
    "    if delta.days == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return delta.days\n",
    "\n",
    "# create a udf for this function\n",
    "get_time_difference_udf = udf(get_time_difference, IntegerType())\n",
    "\n",
    "time_df = time_df.withColumn(\"duration\", get_time_difference_udf(col(\"first_log\"), col(\"last_log\"))) \\\n",
    "            .drop(\"first_log\", \"last_log\").withColumnRenamed(\"churned\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dummy dataframe where each action (About, Thumbs Up, ...) from page is a new column with the number\n",
    "# how often this action appeared in the data for each user\n",
    "dummy_df = valid_events.select(\"userId\", \"page\").groupBy(\"userId\").pivot(\"page\") \\\n",
    "    .count().drop(\"Cancel\", \"Cancellation Confirmation\")\n",
    "# fill null values\n",
    "dummy_df = dummy_df.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_level = valid_events.orderBy(\"log_date\", ascending=False).groupBy(\"userId\").agg(F.first(\"level\").alias('valid_level'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_gender = valid_events.select([\"userId\", \"gender\"]).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the total amount of days the user listened to music\n",
    "songs_per_date = valid_events.withColumn(\"date\", F.to_date(col(\"log_date\"))).where(col(\"page\") == \"NextSong\") \\\n",
    "    .groupBy([\"userId\", \"date\"]).agg(F.lit(1).alias(\"played_music\"))\n",
    "songs_per_day = songs_per_date.groupBy(\"userId\").agg(F.sum(\"played_music\").alias(\"music_days\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join user_df (time_df, dummy_df) with user_level and gender_level and user_length\n",
    "df = time_df.join(dummy_df, on=\"userId\").join(user_level, on=\"userId\") \\\n",
    "    .join(user_gender, on=\"userId\").join(user_length, on=\"userId\").join(songs_per_day, on=\"userId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the actions by the amount of logs or the overall duration of their registration\n",
    "def divide_columns_by(df, columns, value, appendix):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    for name in columns:\n",
    "        new_name = name+\"_\"+appendix\n",
    "        df = df.withColumn(new_name, col(name) / col(value))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_divide = ['music_days', 'About', 'Add Friend', 'Add to Playlist', 'Downgrade', 'Error', 'Help', 'Home',\n",
    "               'Logout', 'NextSong', 'Roll Advert', 'Save Settings', 'Settings', 'Submit Downgrade',\n",
    "               'Submit Upgrade', 'Thumbs Down', 'Thumbs Up', 'Upgrade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_per = divide_columns_by(df, cols_to_divide, \"duration\", \"per_day\")\n",
    "df_model = divide_columns_by(df_per, cols_to_divide, \"log_counts\", \"per_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- label: long (nullable = true)\n",
      " |-- log_counts: long (nullable = false)\n",
      " |-- last_ts: long (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- About: long (nullable = true)\n",
      " |-- Add Friend: long (nullable = true)\n",
      " |-- Add to Playlist: long (nullable = true)\n",
      " |-- Downgrade: long (nullable = true)\n",
      " |-- Error: long (nullable = true)\n",
      " |-- Help: long (nullable = true)\n",
      " |-- Home: long (nullable = true)\n",
      " |-- Logout: long (nullable = true)\n",
      " |-- NextSong: long (nullable = true)\n",
      " |-- Roll Advert: long (nullable = true)\n",
      " |-- Save Settings: long (nullable = true)\n",
      " |-- Settings: long (nullable = true)\n",
      " |-- Submit Downgrade: long (nullable = true)\n",
      " |-- Submit Upgrade: long (nullable = true)\n",
      " |-- Thumbs Down: long (nullable = true)\n",
      " |-- Thumbs Up: long (nullable = true)\n",
      " |-- Upgrade: long (nullable = true)\n",
      " |-- valid_level: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- avg_length: double (nullable = true)\n",
      " |-- music_days: long (nullable = true)\n",
      " |-- music_days_per_day: double (nullable = true)\n",
      " |-- About_per_day: double (nullable = true)\n",
      " |-- Add Friend_per_day: double (nullable = true)\n",
      " |-- Add to Playlist_per_day: double (nullable = true)\n",
      " |-- Downgrade_per_day: double (nullable = true)\n",
      " |-- Error_per_day: double (nullable = true)\n",
      " |-- Help_per_day: double (nullable = true)\n",
      " |-- Home_per_day: double (nullable = true)\n",
      " |-- Logout_per_day: double (nullable = true)\n",
      " |-- NextSong_per_day: double (nullable = true)\n",
      " |-- Roll Advert_per_day: double (nullable = true)\n",
      " |-- Save Settings_per_day: double (nullable = true)\n",
      " |-- Settings_per_day: double (nullable = true)\n",
      " |-- Submit Downgrade_per_day: double (nullable = true)\n",
      " |-- Submit Upgrade_per_day: double (nullable = true)\n",
      " |-- Thumbs Down_per_day: double (nullable = true)\n",
      " |-- Thumbs Up_per_day: double (nullable = true)\n",
      " |-- Upgrade_per_day: double (nullable = true)\n",
      " |-- music_days_per_log: double (nullable = true)\n",
      " |-- About_per_log: double (nullable = true)\n",
      " |-- Add Friend_per_log: double (nullable = true)\n",
      " |-- Add to Playlist_per_log: double (nullable = true)\n",
      " |-- Downgrade_per_log: double (nullable = true)\n",
      " |-- Error_per_log: double (nullable = true)\n",
      " |-- Help_per_log: double (nullable = true)\n",
      " |-- Home_per_log: double (nullable = true)\n",
      " |-- Logout_per_log: double (nullable = true)\n",
      " |-- NextSong_per_log: double (nullable = true)\n",
      " |-- Roll Advert_per_log: double (nullable = true)\n",
      " |-- Save Settings_per_log: double (nullable = true)\n",
      " |-- Settings_per_log: double (nullable = true)\n",
      " |-- Submit Downgrade_per_log: double (nullable = true)\n",
      " |-- Submit Upgrade_per_log: double (nullable = true)\n",
      " |-- Thumbs Down_per_log: double (nullable = true)\n",
      " |-- Thumbs Up_per_log: double (nullable = true)\n",
      " |-- Upgrade_per_log: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_model.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline data\n",
    "numerical_baseline = [\"About\", \"Add Friend\", \"Add to Playlist\", \"Downgrade\", \"Error\",\n",
    "                      \"Help\", \"Home\", \"Login\", \"Logout\", \"NextSong\", \"Register\", \"Roll Advert\", \"Save Settings\",\n",
    "                      \"Settings\", \"Submit Downgrade\", \"Submit Registration\", \"Submit Upgrade\", \"Thumbs Down\",\n",
    "                      \"Thumbs Up\", \"Upgrade\"]\n",
    "columns_baseline = [\"gender\", \"valid_level\"] + numerical_baseline\n",
    "\n",
    "df_baseline = df_model.select(*columns_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>RFE</th>\n",
       "      <th>Logistics</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>LightGBM</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>last_ts</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>duration</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>music_days</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thumbs Down_per_log</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Submit Upgrade_per_day</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Feature  Pearson   RFE  Logistics  Random Forest  LightGBM  \\\n",
       "0                 last_ts     True  True       True           True      True   \n",
       "1                duration     True  True       True           True      True   \n",
       "2              music_days     True  True       True          False      True   \n",
       "3     Thumbs Down_per_log     True  True       True           True     False   \n",
       "4  Submit Upgrade_per_day     True  True       True           True     False   \n",
       "\n",
       "   Total  \n",
       "0      5  \n",
       "1      5  \n",
       "2      4  \n",
       "3      4  \n",
       "4      4  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load feature importance from csv\n",
    "num_feature_df = pd.read_csv(\"data/feature_selection_df.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "num_feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final 22 numerical features selected by feature importance: \n",
      "['last_ts', 'duration', 'music_days', 'Thumbs Down_per_log', 'Submit Upgrade_per_day', 'NextSong_per_log', 'Add Friend_per_day', 'About', 'Submit Downgrade', 'Roll Advert', 'Home_per_day', 'Help_per_day', 'Downgrade', 'Add to Playlist', 'log_counts', 'avg_length', 'Upgrade_per_log', 'Thumbs Up_per_day', 'Settings_per_day', 'Save Settings_per_day', 'Logout_per_log', 'Error']\n"
     ]
    }
   ],
   "source": [
    "# creating a list of features which do not contain any duplicates e.g. not both Thumbs Down_per_log and Thumbs Down\n",
    "important_num_features = list(num_feature_df[num_feature_df[\"Total\"] >= 2][\"Feature\"])\n",
    "unique_num_features = []\n",
    "final_num_features = []\n",
    "for idx, f in enumerate(important_num_features):\n",
    "    if f.find(\"_per_day\") > -1:\n",
    "        f_stripped = f.replace(\"_per_day\", \"\")\n",
    "    elif f.find(\"_per_log\") > -1:\n",
    "        f_stripped = f.replace(\"_per_log\", \"\")\n",
    "    else:\n",
    "        f_stripped = f\n",
    "    # check if feature is already in or not\n",
    "    if f_stripped not in unique_num_features:\n",
    "        unique_num_features.append(f_stripped)\n",
    "        final_num_features.append(f)\n",
    "        \n",
    "print(\"Final {} numerical features selected by feature importance: \\n{}\".format(len(final_num_features), final_num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- valid_level: string (nullable = true)\n",
      " |-- last_ts: long (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- music_days: long (nullable = true)\n",
      " |-- Thumbs Down_per_log: double (nullable = true)\n",
      " |-- Submit Upgrade_per_day: double (nullable = true)\n",
      " |-- NextSong_per_log: double (nullable = true)\n",
      " |-- Add Friend_per_day: double (nullable = true)\n",
      " |-- About: long (nullable = true)\n",
      " |-- Submit Downgrade: long (nullable = true)\n",
      " |-- Roll Advert: long (nullable = true)\n",
      " |-- Home_per_day: double (nullable = true)\n",
      " |-- Help_per_day: double (nullable = true)\n",
      " |-- Downgrade: long (nullable = true)\n",
      " |-- Add to Playlist: long (nullable = true)\n",
      " |-- log_counts: long (nullable = false)\n",
      " |-- avg_length: double (nullable = true)\n",
      " |-- Upgrade_per_log: double (nullable = true)\n",
      " |-- Thumbs Up_per_day: double (nullable = true)\n",
      " |-- Settings_per_day: double (nullable = true)\n",
      " |-- Save Settings_per_day: double (nullable = true)\n",
      " |-- Logout_per_log: double (nullable = true)\n",
      " |-- Error: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = [\"gender\", \"valid_level\"]\n",
    "columns_for_modeling = [\"label\"] +  categorical_columns + final_num_features\n",
    "df_model = df_model.select(*columns_for_modeling)\n",
    "df_model.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no userId should be in here!\n",
    "train = df_model.sampleBy(\"label\", fractions={0: 0.8, 1: 0.5}, seed=42) # randomSplit([0.8, 0.2], seed=42)\n",
    "test = df_model.subtract(train) # for balancing\n",
    "\n",
    "train_baseline = df_baseline.sampleBy(\"label\", fractions={0: 0.8, 1: 0.5}, seed=42)\n",
    "test_baseline = df_baseline.subtract(train_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle categorical columns in pipeline\n",
    "indexers = []\n",
    "\n",
    "for cat in categorical_columns:\n",
    "    indexers.append(StringIndexer(inputCol = cat, outputCol = \"{}_indexed\".format(cat)))\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=[indexer.getOutputCol() for indexer in indexers],\n",
    "    outputCols=[\"{}_encoded\".format(indexer.getOutputCol()) for indexer in indexers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical_cols = [col.name for col in df.schema.fields if col.dataType != StringType()]\n",
    "# numerical_cols.remove(\"label\")\n",
    "# numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_assembler = VectorAssembler(inputCols=final_num_features, outputCol=\"numeric_vectorized\")\n",
    "#Lets scale the data\n",
    "scaler = StandardScaler(inputCol = \"numeric_vectorized\", outputCol = \"numeric_scaled\", withStd = True, withMean = True)\n",
    "\n",
    "\n",
    "#create final VectorAssembler to push data to ML models\n",
    "assembler = VectorAssembler(inputCols=[\"numeric_scaled\"] + encoder.getOutputCols(), outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forst Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "pipeline_rf = Pipeline(stages= indexers + [encoder, numeric_assembler, scaler, assembler, model_rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid_rf = ParamGridBuilder() \\\n",
    "        .addGrid(model_rf.numTrees, [20,75]) \\\n",
    "        .addGrid(model_rf.maxDepth, [10, 20]) \\\n",
    "        .build() \n",
    "\n",
    "\n",
    "crossval_rf = CrossValidator(estimator=pipeline_rf,\n",
    "                          estimatorParamMaps=paramGrid_rf,\n",
    "                          evaluator=MulticlassClassificationEvaluator(metricName=\"f1\"),\n",
    "                          numFolds=3) # here you can set parallelism parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_time = time.time()\n",
    "#cvModel_rf = crossval_rf.fit(train)\n",
    "#end_time = time.time()\n",
    "#print(\"Fitting the model took {} s.\".format(round(end_time - start_time,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate log regression model\n",
    "model_lr = LogisticRegression()\n",
    "\n",
    "# Make pipeline for lr\n",
    "pipeline_lr = Pipeline(stages= indexers + [encoder, numeric_assembler, scaler, assembler, model_lr])\n",
    "\n",
    "# Grid Search Params\n",
    "paramGrid_lr = ParamGridBuilder() \\\n",
    "    .addGrid(model_lr.maxIter, [10, 20]) \\\n",
    "    .addGrid(model_lr.elasticNetParam, [0, 0.5]) \\\n",
    "    .addGrid(model_lr.regParam,[0.1, 1]) \\\n",
    "    .build()\n",
    "\n",
    "crossval_lr = CrossValidator(estimator=pipeline_lr,\n",
    "                          estimatorParamMaps=paramGrid_lr,\n",
    "                          evaluator=MulticlassClassificationEvaluator(metricName=\"f1\"),\n",
    "                          numFolds=3) # here you can set parallelism parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBTC Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate log regression model\n",
    "model_gbtc = GBTClassifier()\n",
    "\n",
    "# Make pipeline for lr\n",
    "pipeline_gbtc = Pipeline(stages= indexers + [encoder, numeric_assembler, scaler, assembler, model_gbtc])\n",
    "\n",
    "# Grid Search Params\n",
    "paramGrid_gbtc = ParamGridBuilder() \\\n",
    "    .addGrid(model_gbtc.maxIter, [10, 12]) \\\n",
    "    .build()\n",
    "\n",
    "crossval_gbtc = CrossValidator(estimator=pipeline_gbtc,\n",
    "                          estimatorParamMaps=paramGrid_gbtc,\n",
    "                          evaluator=MulticlassClassificationEvaluator(metricName=\"f1\"),\n",
    "                          numFolds=3) # here you can set parallelism parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM: either use the one from mmlspark or LGBMClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate \n",
    "# from mmlspark.lightgbm import LightGBMClassifier\n",
    "#model_lgbm = LGBMClassifier()\n",
    "\n",
    "# Make pipeline for lr\n",
    "#pipeline_lgbm = Pipeline(stages= indexers + [encoder, numeric_assembler, scaler, assembler, model_lgbm])\n",
    "\n",
    "# Grid Search Params\n",
    "#paramGrid_lgbm = ParamGridBuilder() \\\n",
    "#    .addGrid(model_lgbm.learningRate, [0.1, 0.5]) \\\n",
    "#    .addGrid(model_lgbm.numLeaves, [31, 76]) \\\n",
    "#    .addGrid(model_lgbm.numIterations, [100]) \\\n",
    "#    .addGrid(model_lgbm.objective, [\"binary\"]) \\\n",
    "#    .build()\n",
    "\n",
    "#crossval = CrossValidator(estimator=pipeline_lgbm,\n",
    "#                          estimatorParamMaps=paramGrid_lgbm,\n",
    "#                          evaluator=MulticlassClassificationEvaluator(metricName=\"f1\"),\n",
    "#                          numFolds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_assembler_baseline = VectorAssembler(inputCols=numerical_baseline, outputCol=\"numeric_vectorized\")\n",
    "#Lets scale the data\n",
    "scaler_baseline = StandardScaler(inputCol = \"numeric_vectorized\", outputCol = \"numeric_scaled\", withStd = True, withMean = True)\n",
    "\n",
    "\n",
    "#create final VectorAssembler to push data to ML models\n",
    "assembler = VectorAssembler(inputCols=[\"numeric_scaled\"] + encoder.getOutputCols(), outputCol=\"features\")\n",
    "model_baseline = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "pipeline_baseline = Pipeline(stages= indexers + [encoder, numeric_assembler_baseline, scaler_baseline, assembler, model_rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_crossval(crossval, train):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    start_time = time.time() # start\n",
    "    cv_model = crossval.fit(train) # fit\n",
    "    end_time = time.time() # end\n",
    "    \n",
    "    print(\"Fitting the model took {} s.\".format(round(end_time - start_time,2)))\n",
    "    return cv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train, test, metric = 'f1'):\n",
    "    \"\"\"TODO\"\"\"\n",
    "    # init evaluator\n",
    "    evaluator = MulticlassClassificationEvaluator(metricName = metric)\n",
    "    \n",
    "    # make predictions\n",
    "    prediction_result_train = model.transform(train)\n",
    "    prediction_result_test = model.transform(test)\n",
    "    \n",
    "    # calcualte the scores\n",
    "    score_train = evaluator.evaluate(prediction_result_train)\n",
    "    score_test = evaluator.evaluate(prediction_result_test)\n",
    "    print(\"{} score on training data is {}\".format(metric, score_train))\n",
    "    print(\"{} score on test data is {}\".format(metric, score_test))\n",
    "    \n",
    "    # feature importance\n",
    "    #importance = {}\n",
    "    #for i in range(len(model.stages[-1].featureImportances)):\n",
    "    #    importance[features_df.columns[i]] = model.stages[-1].featureImportances[i]\n",
    "    # \n",
    "    #sorted_importance = {k: v for k, v in sorted(importance.items(), key=lambda item: item[1])}\n",
    "    #print(sorted_importance)\n",
    "    try:\n",
    "        print(model.stages[-1].featureImportances)\n",
    "        try:\n",
    "            features = prediction_result_test.select(\"features\").distinct().collect()\n",
    "            print(features)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            numeric_metadata = prediction_result_test.select(\"features\").schema[0].metadata.get('ml_attr').get('attrs').get('numeric')\n",
    "            binary_metadata = prediction_result_test.select(\"features\").schema[0].metadata.get('ml_attr').get('attrs').get('binary')\n",
    "\n",
    "            merge_list = numeric_metadata + binary_metadata\n",
    "            print(merge_list)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return score_train, score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Models and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Random-Forst model ...\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Field \"Thumbs Down_per_log\" does not exist.\\nAvailable fields: userId, label, log_counts, last_ts, duration, About, Add Friend, Add to Playlist, Downgrade, Error, Help, Home, Logout, NextSong, Roll Advert, Save Settings, Settings, Submit Downgrade, Submit Upgrade, Thumbs Down, Thumbs Up, Upgrade, valid_level, gender, avg_length, music_days, music_days_per_day, About_per_day, Add Friend_per_day, Add to Playlist_per_day, Downgrade_per_day, Error_per_day, Help_per_day, Home_per_day, Logout_per_day, NextSong_per_day, Roll Advert_per_day, Save Settings_per_day, Settings_per_day, Submit Downgrade_per_day, Submit Upgrade_per_day, Thumbs Down_per_day, Thumbs Up_per_day, Upgrade_per_day, CrossValidator_ba8ed13f5ea0_rand, gender_indexed, valid_level_indexed, gender_indexed_encoded, valid_level_indexed_encoded'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\FRANZK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FRANZK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o4852.transform.\n: java.lang.IllegalArgumentException: Field \"Thumbs Down_per_log\" does not exist.\nAvailable fields: userId, label, log_counts, last_ts, duration, About, Add Friend, Add to Playlist, Downgrade, Error, Help, Home, Logout, NextSong, Roll Advert, Save Settings, Settings, Submit Downgrade, Submit Upgrade, Thumbs Down, Thumbs Up, Upgrade, valid_level, gender, avg_length, music_days, music_days_per_day, About_per_day, Add Friend_per_day, Add to Playlist_per_day, Downgrade_per_day, Error_per_day, Help_per_day, Home_per_day, Logout_per_day, NextSong_per_day, Roll Advert_per_day, Save Settings_per_day, Settings_per_day, Submit Downgrade_per_day, Submit Upgrade_per_day, Thumbs Down_per_day, Thumbs Up_per_day, Upgrade_per_day, CrossValidator_ba8ed13f5ea0_rand, gender_indexed, valid_level_indexed, gender_indexed_encoded, valid_level_indexed_encoded\r\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\r\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\r\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\r\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\r\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:273)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$6.apply(VectorAssembler.scala:162)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$6.apply(VectorAssembler.scala:161)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\r\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:186)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:161)\r\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:86)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-151-4e5fb4f49312>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fitting {} model ...\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mcv_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_crossval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcrossval_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcv_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;31m# evaluate it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mscore_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-150-9d082b118608>\u001b[0m in \u001b[0;36mfit_crossval\u001b[1;34m(crossval, train)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"\"\"TODO\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# start\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mcv_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrossval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# end\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FRANZK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mC:\\Users\\FRANZK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\ml\\tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meva\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FRANZK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    746\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    749\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m     \u001b[0m__next__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m                    \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FRANZK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mworker\u001b[1;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mjob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FRANZK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\ml\\tuning.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meva\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FRANZK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\ml\\tuning.py\u001b[0m in \u001b[0;36msingleTask\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msingleTask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelIter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mmetric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meva\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcollectSubModel\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FRANZK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No models remaining.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcounter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitSingleModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FRANZK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfitSingleModel\u001b[1;34m(index)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfitSingleModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparamMaps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_FitMultipleIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfitSingleModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparamMaps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FRANZK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FRANZK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\ml\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m                     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# must be an Estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FRANZK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    171\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FRANZK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FRANZK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\FRANZK1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: 'Field \"Thumbs Down_per_log\" does not exist.\\nAvailable fields: userId, label, log_counts, last_ts, duration, About, Add Friend, Add to Playlist, Downgrade, Error, Help, Home, Logout, NextSong, Roll Advert, Save Settings, Settings, Submit Downgrade, Submit Upgrade, Thumbs Down, Thumbs Up, Upgrade, valid_level, gender, avg_length, music_days, music_days_per_day, About_per_day, Add Friend_per_day, Add to Playlist_per_day, Downgrade_per_day, Error_per_day, Help_per_day, Home_per_day, Logout_per_day, NextSong_per_day, Roll Advert_per_day, Save Settings_per_day, Settings_per_day, Submit Downgrade_per_day, Submit Upgrade_per_day, Thumbs Down_per_day, Thumbs Up_per_day, Upgrade_per_day, CrossValidator_ba8ed13f5ea0_rand, gender_indexed, valid_level_indexed, gender_indexed_encoded, valid_level_indexed_encoded'"
     ]
    }
   ],
   "source": [
    "crossval_dict = {}\n",
    "crossval_dict[\"Baseline\"] = crossval_baseline\n",
    "crossval_dict[\"Random-Forst\"] = crossval_rf\n",
    "crossval_dict[\"Logistic Regression\"] = crossval_lr\n",
    "crossval_dict[\"GBTC\"] = crossval_gbtc\n",
    "result_dict = {}\n",
    "if len(crossval_dict) > 1:\n",
    "    # worth caching\n",
    "    train.cache()\n",
    "    test.cache()\n",
    "\n",
    "# tuned models\n",
    "for cv_key in crossval_dict:\n",
    "    # fit the model\n",
    "    print(\"Fitting {} model ...\".format(cv_key))\n",
    "    if cv_key == \"Baseline\":\n",
    "        # fit model\n",
    "        cv_model = fit_crossval(crossval_dict[cv_key], train_baseline)\n",
    "        # evaluate model\n",
    "        score_train, score_test = evaluate_model(cv_model.bestModel, train_baseline, test_baseline)\n",
    "    else:\n",
    "        # fit model\n",
    "        cv_model = fit_crossval(crossval_dict[cv_key], train)\n",
    "        # evaluate it\n",
    "        score_train, score_test = evaluate_model(cv_model.bestModel, train, test)\n",
    "        \n",
    "    result_dict[cv_key] = [score_train, score_test]\n",
    "\n",
    "print(\"-------------Result Summary--------------\")\n",
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
